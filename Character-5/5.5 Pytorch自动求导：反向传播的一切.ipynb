{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93faba81-2c1d-4755-ba89-3b99da7e55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79dc6d-e285-4a4b-9678-26bb1dd7711c",
   "metadata": {},
   "source": [
    "## 5.5.1 自动计算梯度"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb6b4645-8360-424e-8e92-d6f37f2001e9",
   "metadata": {},
   "source": [
    "1.反向传播是一个自动计算梯度的过程\n",
    "2.在梯度计算的过程中，每一个参数的梯度值会储存在.grad属性中用于参数更新的计算中，如果不及时清零的话，不会对反向传播过程产生影响，但是新的梯度会直接和之前存储的\n",
    "  梯度相加，导致梯度计算出错\n",
    "3.with no_grad()代表其中的有关参数的计算不会纳入反向传播的梯度计算中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3190b83e-21c1-4539-b75a-40fe22b664be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.应用自动导\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7273aabf-930c-48f9-a269-a37317b3dc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.使用grad属性\n",
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ea63ab-0ca0-4806-b6ac-a31e5c5ffc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a59564b7-b552-4e8d-8633-5fb2df7a4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.累加梯度函数\n",
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040c65d7-cf2b-4463-91c1-82db5e71da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss {loss}\")\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6896afcc-1ccc-4050-a987-46af605a8261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115051269531\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 2000, Loss 2.957697868347168\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 3000, Loss 2.9286484718322754\n",
      "Epoch 3500, Loss 2.9278297424316406\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=5000, learning_rate=1e-2, params=torch.tensor([1.0, 0.0], requires_grad=True), t_u=t_un, t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d27484-f7a7-4e96-9f6d-2b05669dcf71",
   "metadata": {},
   "source": [
    "## 5.5.2 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9467022-fe8b-4f93-bacf-8f2ea72607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ced1dbd-a79b-45fb-a409-9672367d1195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a94d85fa-e5ec-43cc-b621-973b33d9716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "639a2d07-268f-4aaa-b2b9-954de94d4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-44.1730,  -0.8260], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bef8cc5e-7303-46a6-8c9f-c60d51f9e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not epoch % 500:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f7de6cd-fd19-4d9c-8ef7-39236c997861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 57.37331771850586\n",
      "Epoch: 1000, Loss: 44.78495788574219\n",
      "Epoch: 1500, Loss: 37.90142059326172\n",
      "Epoch: 2000, Loss: 34.128047943115234\n",
      "Epoch: 2500, Loss: 32.05025100708008\n",
      "Epoch: 3000, Loss: 30.896940231323242\n",
      "Epoch: 3500, Loss: 30.24767303466797\n",
      "Epoch: 4000, Loss: 29.87321662902832\n",
      "Epoch: 4500, Loss: 29.648618698120117\n",
      "Epoch: 5000, Loss: 29.505748748779297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2361, 0.0655], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], learning_rate)\n",
    "\n",
    "training_loop(5000, optimizer, params, t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77421871-8586-4ea3-a128-832c92bdb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 7.6128997802734375\n",
      "Epoch: 1000, Loss: 3.086698293685913\n",
      "Epoch: 1500, Loss: 2.9285776615142822\n",
      "Epoch: 2000, Loss: 2.9276463985443115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试其他优化器\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer=optim.Adam([params], learning_rate)\n",
    "training_loop(2000, optimizer, params, t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06c92c-3d0a-4356-aaf3-bd570fca7a5a",
   "metadata": {},
   "source": [
    "## 5.5.3 训练、验证和过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d09445e-7c24-4957-a353-7bccb1de4197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9, 1, 0, 4, 6, 8, 5, 7, 2]), tensor([ 3, 10]), 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 评估训练损失\n",
    "# 2. 推广到验证集\n",
    "# 3. 分割数据集\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)  # 将一个张量的元素打乱，相当于找到一种办法将元素索引重新排列\n",
    "\n",
    "# 获得训练集和验证集的索引\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfcbdc9c-f996-46bb-9451-3121bfc5c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6cd5b9b5-447b-43af-8b35-3cf1e6d4406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch < 4 or epoch % 500 ==0:\n",
    "            print(f\"Epoch: {epoch}, Training Loss: {train_loss.item():.4f}, Val loss: {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55b0aced-e05b-4473-8aca-c3c69fab7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 54.4696, Val loss: 196.8908\n",
      "Epoch: 2, Training Loss: 29.6949, Val loss: 114.1472\n",
      "Epoch: 3, Training Loss: 24.3173, Val loss: 86.2842\n",
      "Epoch: 500, Training Loss: 6.9291, Val loss: 25.0236\n",
      "Epoch: 1000, Training Loss: 3.4149, Val loss: 12.7145\n",
      "Epoch: 1500, Training Loss: 2.6488, Val loss: 8.6820\n",
      "Epoch: 2000, Training Loss: 2.4817, Val loss: 7.1729\n",
      "Epoch: 2500, Training Loss: 2.4453, Val loss: 6.5498\n",
      "Epoch: 3000, Training Loss: 2.4374, Val loss: 6.2767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.0879, -15.7776], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], learning_rate)\n",
    "training_loop(3000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfc1c4-8812-4a62-b979-43f2f66b7ba2",
   "metadata": {},
   "source": [
    "## 5.5.4 自动求导更新及关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "739656d7-9e2a-497b-93e5-f80e4dbc441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_t_p = modele(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_pl, val_t_c)\n",
    "            assert val_loss.requires_grag == False\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7911d-5835-47f6-a214-d97f1ec6851b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
